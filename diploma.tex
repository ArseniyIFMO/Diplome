    \documentclass[ 12pt,x11names]{article}
    \usepackage{float}
    \usepackage[utf8x]{inputenc}
    \usepackage[T2A]{fontenc}
    \usepackage[russian]{babel}
    \usepackage{amsfonts}
    \usepackage{amssymb,amsmath,color}
    \usepackage[14pt]{extsizes}
    \usepackage{wrapfig}
    \usepackage{pgfplots}
    \usepackage{indentfirst}
 \usepackage{geometry}
 \usepackage{algorithm2e}
% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{12} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal

% Custom colors
\usepackage{color}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}

\usepackage{listings}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
basicstyle=\ttm,
otherkeywords={self},             % Add keywords here
keywordstyle=\ttb\color{deepblue},
emph={MyClass,__init__},          % Custom highlighting
emphstyle=\ttb\color{deepred},    % Custom highlighting style
stringstyle=\color{deepgreen},
frame=tb,                         % Any extra options here
showstringspaces=false            %
}}



% Python environment
\lstnewenvironment{python}[1][]
{
\pythonstyle
\lstset{#1}
}
{}

% Python for external files
\newcommand\pythonexternal[2][]{{
\pythonstyle
\lstinputlisting[#1]{#2}}}

% Python for inline
\newcommand\pythoninline[1]{{\pythonstyle\lstinline!#1!}}
    \begin{document}
    \tableofcontents % сгенерировать оглавление
    \clearpage
    \section{Введение}    \noindent
    В данной работе мы будем искать метастабильные состояния магнитной системы\\
    Характеризация магнитных материалов начинается с  поиска метастабильного состояния системы.
    В данной работе будет решаться задача поиска такого состояния.\\
        Метастабильное состояние - это состояние системы, в котором энергия минимальна, и, соответственно, в работе требуется находить минимальную энергию некоторой магнитной системы.\\
    Энергия магнитной системы выражается некоторым функционалом, зависящим от положения каждого из скирмионов, причем на положения скирмионов накладываются некоторые ограничения. \\Соответственно, в данной работе будет произведено иследование различных методов поиска минимума таких функционалов.\\
    Проблемматика исследований:\\
     Так как в силу ограничений, накладываемых на скирмионы, использование стандартных алгоритмов невозможно, нам придется придумывать какие то новые алгоритмы, не существующие в данный момент.\\
    В данной работе я не только приведу пример алгоритма, решающего данную задачу, но и   рассмотрю различне методы выполнения данной задачи, реализую их на языке Python, c использованием бибилотек Numpy, сравню их скорость сходимости, сделаю вывод о их работоспособности в условиях данной задачи, времени работы и так далее.\\
    Актуальность задачи:\\
    Данная задача очень важна в теоретической физике, особенно важна скорость сходимости, так как в реальных физических системах количество частиц может быть очень велико, и ассимтотика очень важна прирешении данной задачи.\\
    Я собираюсь использовать градиентный спуск, рассмотрю скорость и точность сходимости в зависимости от параметров, различные его вариации, а так же многосеточные методы для решения данной задачи.\\
     Практическая значимость полученных результатов состоит в возможности экономии времени других исследователей при решении данной задачи, мы найдем какой алгоритм поиска метастабильного состояния является наиболее оптимальным и точным, и, соответственно, убережем других исседователей от выборов неправильных или неэффективных инструментов.\\

    \newpage
    \section{Глава 1 Теоретическая часть работы}
    Пусть наша магнитная система будет размера $p \times  q$. Тогда $n =  p * q$.
    Полная энергия магнитной системы состоит из:
    Тогда энергия магнитной системы описывается как:
    \begin{center}
    $E(S) = - \frac{1}{2}S^T*A*S + B*S$,
    \end{center}
    где $A$ - матрица $n * n$, $B$ - матрица $1 \times n$,
    каждый элемент которой - трехмерный вектор $(x, y, z)$
    , a $S$ -- матрица $1*n$, каждый элемент которой - трехмерный вектор $S_i = (x, y, z)$ - вектор магнитного момента $i$-го спина. \\
    Матрица S и описывает состояние магнитной системы.
    Причем, требуется что бы норма каждого из векторов была равна $1$.Иными словами, должно соблюдаться условие:\\
     \begin{center}
     $x^2 + y ^ 2 + z ^ 2 = 1, \|S_{i, j}\| = 1$\\
    \end{center}
    Полная энергия магнитной системы состоит из энергии обмена между соседними скирмионами в сетке,  антисимметричного обменного взаимодействия или взаимодействием Дзялошинского — Мория, а так же анизотропии.\\
    В нашей задаче:
    \begin{equation*}
    A = \left(
    \begin{array}{ccccс}
    Kzz^T & J + [Dx]_\times & 0  & \ldots & 0\\
    J - [Dx]_\times & Kzz^T & J + [Dx]_\times &\ldots & 0\\
    0 & J - [Dx]_\times & Kzz^T &\ldots & 0\\
    \vdots &\vdots &\vdots &\ddots & \vdots\\
    0 & 0 &  0 &\ldots & Kzz^T
    \end{array}
    \right)
    \end{equation*}\\
     \begin{equation*}
    B = \left(
    \begin{array}{c}
    H_1\\
    H_2\\
    H_3\\
    \vdots\\
    H_n
    \end{array}
    \right)
    \end{equation*}\\
    То есть  \begin{center} $A_{i, i} = -1 \\ A_{i - 1, i} =  J + [D*x]_\times \\
    A_{i+1, i} =  J - [D*x]_\times \\
    A_{i + p, i} =  J + [D*y]_\times \\
    A_{i - p , i} =  J -  [D*x]_\times$
     \end{center}, потому что если переводить исходную решетку в вектор, то взаимодействовать будут соседние в решетке.
    В конечном итоге, нам требуется минимизировать данный функционал.\\
    Записать энергию $E(S)$ можно тогда по другому.
    Согласно статье [1], энергия магнитной системы для модели Гизенберга записывается как:\\
    $x = (1,0, 0), y = (0,1, 0), z =(0, 0 , 1)$\\
        \begin{center}
    $E(S) = \displaystyle{\sum_{i, j}} - J (S_{i - 1, j} + S_{i + 1, j} + S_{i, j - 1} + S_{i, j + 1}) \cdot S_{i,j} + \\
    D  (-S_{i - 1, j} \times x  + S_{i + 1, j} \times x - S_{i, j - 1} \times y - S_{i, j + 1} \times y)\cdot S_{i,j}
    + \\
    K  (z \cdot S_{i,j}) ^ 2$,\\
    \end{center}[1]
    где
     \begin{center}
    $\displaystyle{\sum_{i, j}} J* (S_{i - 1, j} + S_{i + 1, j} + S_{i, j - 1} + S_{i, j + 1}) \cdot S_{i,j}$\end{center} -- энергия обмена, а \begin{center}
    $\displaystyle{\sum_{i, j}} D * (-S_{i - 1, j} \times x  + S_{i + 1, j} \times x - S_{i, j - 1} \times y - S_{i, j + 1} \times y)$\end{center}  -- взаимодействие Дзялошински - Мория, а - \begin{center}
    $\displaystyle{\sum_{i, j}} K * (z \cdot S_{i,j}) ^ 2$\end{center} - энергия анизотропии\\
    Обычно, таки задачи решаются градиентным спуском, однако, в данной ситуации он не применим напрямую из за ограничений на норму спинов.
    Далее,мы рассмотрим несколько алгоритмов, которыми мы попытаемся решить данную задачу.
    \newpage
    \section{Методы решения}
    \subsection{Метод 1}
    Первый метод которым можно воспользоваться  - градиентный спуск[2]. Градиент по скирмиону на позиции $i, j$ будет равен:
      \begin{center}
    $\nabla E(S)_{i, j} = J * (S_{i + 1,j} + S_{i - 1,j} + S_{i, j + 1} + S_{i, j - 1})
        + \\ D  ( (S_{i + 1,j} \times x) + (S_{i, j + 1} \times y) -  (S_{i - 1, j} \times x) - (S_{i, j - 1} \times y) )\\
        + 2 * z * K * (z \cdot S_{i, j})$\\
          \end{center}
    Будем  итерироваться как в градиентнтом спуске:
    \begin{center}
    $S_{i, j} = S_{i, j} - \alpha * \nabla E(S)_{i, j}$.\\
    \end{center}
    Однако, очевидно, после такого шага может  нарушится условие $\|S_{i, j}\| = 1$.\\
    Тогда, будем действовать следующим образом. Сначала \\
    \begin{center}
    $S_{i, j} = S_{i, j} - \alpha * \nabla E(S)_{i, j}$
    \end{center}, а потом отнормируем каждый элемент, то есть \begin{center}$S_{i, j} = \frac{S_{i, j}}{\|S_{i, j}\|}$.\end{center}\\
    Тогда градиент с учетом нормировки будет равен:\\
    \begin{center}
    $\Delta_{i, j} = \nabla E(S)_{i, j} - S_{i, j} \cdot \nabla E(S)_{i, j} * S_{i, j}$\\
    $\Delta = max(\Delta_{i, j}).$\\
    \end{center}
    Собственно, норма этого градиента и будет условием остановки.\\
    ($\| \Delta \|< \varepsilon$)
    В этом варианте берется $\alpha = 0.001$, однако я далее проанализирую как меняется скороть сходимости  с различными $\alpha$
    \begin{algorithm}[H]
		\SetAlgoLined
		\KwData{Входные данные}
		\KwResult{Как прочитать книгу }
		\alpha = 0.001;\\
		$ \Delta  = 1$\\
		\While{($ \Delta > \varepsilon$)}{
		    $ \Delta  =0$\\
		    \For{ i = 1 to n}{
		        \For{ j = 1 to n}{
		            newS[i][j] = S[i][j] - \alpha * \nabla E(S)_{i,j}\\
		            $\Delta = max(\Delta, \|\nabla S_{i, j} - \nabla S_{i, j} \cdot E(S)_{i, j} * S_{i, j}\|)$\\
	            }
	        }
	        S = newS\\
	        normalize(S)
		}
	\caption{Метод 1}
    \end{algorithm}
    \subsection{Метод 2}
    Так же, я  отдельно попробую метод, в котором  $\alpha$ не постоянная, а каждый раз будет равна новому числу
     $\alpha  = \frac{\| \nabla S^k \|^2}{\| \nabla S^{k-1} \|^2}$.
    Сущетвует несколько способов задать критерий остановки: например, сделать фиксированное число итераций, либо делать итерации пока  ($\| \Delta \|> \varepsilon$)\\
    \begin{algorithm}[H]
		\SetAlgoLined
		\KwData{Входные данные}
		\KwResult{Как прочитать книгу }
		\alpha = 0.001;\\
		\While{$\Delta > \varepsilon$}{
		     $\alpha  = \frac{\| \nabla S^k \|^2}{\| \nabla S^{k-1} \|^2}$\\
		     $ \Delta  =0$\\
		    \For{ i = 1 to n}{
		        \For{ j = 1 to n}{
		            newS[i][j] = S[i][j] - \alpha * \nabla E(S)_{i,j}\\
	                 $\Delta = max(\Delta, \|\nabla S_{i, j} - \nabla S_{i, j} \cdot E(S)_{i, j} * S_{i, j}\|)$\\
	            }
	        }
	        S = newS\\
	        normalize(S)
		}
	\caption{Метод 1}
    \end{algorithm}
    \subsection{Метод 3}
    Воспользуемся методом сопряженных градиентов.\\
    Сперва опробуем метод Флетчера - Риза[5]. В нем:
    \begin{center}
    $ x_0= \nabla E(S^0)\\
    S^{(k + 1)}_{i, j} = S^{k}_{i, j} + \alpha_k * x_k $,\\
     $\beta ^{k +1} = \frac{ (\nabla E(x^k) \cdot \nabla E(S^k))} { (\nabla E(x^k) \cdot x^k)}$\\
    $\alpha_k =\frac{\nabla E(S^k) \cdot x^k}{\nabla E(x^k) \cdot x^k}$\\
    $x_{k + 1}= \nabla E(S^k) + \beta ^ {k + 1} x_k$\\
   .
    \end{center}
    Будем считать, что $\Delta^0=(0,0,0)$\\

    \begin{algorithm}[H]
		\SetAlgoLined
		\KwData{Входные данные}
		\KwResult{Как прочитать книгу }
		\alpha = 0.001;\\
		\While{$MyDelta > \varepsilon$}{
		     $\omega  = \frac{\| \nabla S^k \|^2}{\| \nabla S^{k-1} \|^2}$\\
		    \For{ i = 1 to n}{
		        \For{ j = 1 to n}{
		              S^{(k + 1)}_{i, j} = S^{k}_{i, j} + \alpha_k * x_k $,\\
                      $\beta ^{k +1} = \frac{ (\nabla E(x^k) \cdot \nabla E(S^k))} { (\nabla E(x^k) \cdot x^k)}$\\
                         $\alpha_k =\frac{\nabla E(S^k) \cdot x^k}{\nabla E(x^k) \cdot x^k}$\\
                        $x_{k + 1}= \nabla E(S^k) + \beta ^ {k + 1} x_k$\\
	            }
	        }
	        S = newS\\
	        normalize(S)

		}
	\caption{Метод 1}
    \end{algorithm}
    \subsection{Метод 4}
    Попробуем так - же метод, в котором направление шага на $k$ шаге зависит от направления градиента на $k - 1$ шаге\\
      \begin{center}
     $\Delta^k = -\nabla S_{i, j} + \omega * \Delta ^  {k-1} , \\
		            newS[i][j] = S[i][j] - \alpha * \Delta^k $\\
     \end{center}
    \begin{algorithm}[H]
		\SetAlgoLined
		\KwData{Входные данные}
		\KwResult{Как прочитать книгу }
		\alpha = 0.001;\\
		\While{$MyDelta > \varepsilon$}{
		     $\omega  = \frac{\| \nabla S^k \|^2}{\| \nabla S^{k-1} \|^2}$\\
		    \For{ i = 1 to n}{
		        \For{ j = 1 to n}{
		             $\Delta^k = -\nabla S_{i, j} + \omega * \Delta ^  {k-1} $, \\
		            newS[i][j] = S[i][j] - \alpha * \Delta^k \\
		            $MyDelta = max( MyDelta, \|\nabla S_{i, j} - \nabla S_{i, j} \cdot E(S)_{i, j} * S_{i, j}\|)$\\
	            }
	        }
	        S = newS\\
	        normalize(S)

		}
	\caption{Метод 1}
    \end{algorithm}
    \newpage
    \section{
    Глава 2
    Практическая часть}
    \subsection{Метод 1}
    Приведу реализацию алгоритма на языке и сравним их быстродействие.
    Для реализации данных алгоритмов испоьзуем язык Python, а так - же библиотеку numpy.Язык python и библиотека numpy  были выбранны как наиболее удобные[3][4]
    Реализация с фиксированным $\alpha$\\
\begin{python}
    #Calculate gradient
    def grad(i, j):
        tmp = J * (S[i + 1][j] + S[i - 1][j] +
        +S[i][j + 1] + S[i][j - 1])
        tmp2 = D * (np.cross(S[i + 1][j], x)
        +np.cross(S[i][j + 1], y)
        - np.cross(S[i - 1][j], x)
        - np.cross(S[i][j - 1], y))
        tmp3 = 2 * z * K * np.dot  (z, S[i][j]).item()
        res = - tmp + tmp2 - tmp3
        return res

    #Calculate energy
    def E():
        res = 0
        for i in range(1, SX + 1):
            for j in range(1, SY + 1):
                tmp = J * np.dot((S[i + 1][j] + S[i - 1][j] +
                S[i][j + 1] + S[i][j - 1]), S[i][j])
                tmp2 = D * (
                + np.dot(np.cross(S[i + 1][j], S[i][j]), x)
                + np.dot(np.cross(S[i][j + 1], S[i][j]), y)
                - np.dot(np.cross(S[i - 1][j], S[i][j]), x)
                - np.dot(np.cross(S[i][j - 1], S[i][j]), y))
                tmp3 = K * (np.dot(z, S[i][j]) ** 2)
                res = res - tmp / 2 - tmp2 / 2 - tmp3
        return res

    #Normalise each vector
    def normalize():
    for i in range(1, SX + 1):
        for j in range(1, SY + 1):
            norm = S[i][j][0] * S[i][j][0] +
                   S[i][j][1] * S[i][j][1] +
                   S[i][j][2] * S[i][j][2]
            norm = np.sqrt(norm)
            S[i][j][0] = S[i][j][0] / norm
            S[i][j][1] = S[i][j][1] / norm
            S[i][j][2] = S[i][j][2] / norm


    #Define size of grid
    SX = 4

    #Define y size of grid
    SY = 5

    x = np.array([1.0, 0.0, 0.0])
    y = np.array([0.0, 1.0, 0.0])
    z = np.array([0.0, 0.0, 1.0])

    #Make random starting skirmions
    S = np.random.randn(SX + 2, SY + 2, 3)

    for i in range(0, SX + 2):
        S[i][0] = np.array([0, 0, 0])
        S[i][SY + 1] = np.array([0, 0, 0])

    for i in range(0, SY + 2):
        S[0][i] = np.array([0, 0, 0])
        S[SX + 1][i] = np.array([0, 0, 0])

    #Anisotropy constant
    K = 0.4

    #Exchange energy
    D = 0.35

    #Dzyaloshinskii-Moriya interaction
    J = 1

    #Length of step
    step = 0.1


    for k in range(0, 10000):
        newS = np.zeros_like(S)
        maxNorm = 0
        for i in range(1, SX + 1):
            for j in range(1, SY + 1):
                #Calculte gradient
                g = grad(i,  j)

                #Calculate projection ofgradientto sphere
                projGradOnS = np.dot(S[i][j], g)

                g = g - projGradOnS * S[i][j]
                maxNorm = np.maximum(maxNorm, np.linalg.norm(g))

                #Make step in gradient direction
                newS[i][j] = S[i][j] - step * grad(i, j)

        S = newS

        #normalize all vectors
        normalize()

print(E())
\end{python}
 \subsection{Метод 2}
 Можно попробоватьреализовать обычный градиентный спуск, но не с фиксированным $\alpha$, а с $\alpha = \frac{\| \nabla S^k \|^2}{\| \nabla S^{k-1} \|^2}$\\
  \begin{python}
#Define  x size of grid
SX = 4
#Define y size of grid
SY = 5

x = np.array([1.0, 0.0, 0.0])
y = np.array([0.0, 1.0, 0.0])
z = np.array([0.0, 0.0, 1.0])

#Make random starting skirmions
S = np.random.randn(SX + 2, SY + 2, 3)

for i in range(0, SX + 2):
    S[i][0] = np.array([0, 0, 0])
    S[i][SY + 1] = np.array([0, 0, 0])

for i in range(0, SY + 2):
    S[0][i] = np.array([0, 0, 0])
    S[SX + 1][i] = np.array([0, 0, 0])

#Anisotropy constant
K = 0.4

#Exchange energy
D = 0.35

#Dzyaloshinskii-Moriya interaction
J = 1

#Length of step
step = 0.1

def normalPrintS():
    for i in range(0, SX + 2):
        print()
        for j in range(0, SY + 2):
            print(S[i][j], end=" ")
    print()

#Calculate gradient
def grad(i, j):
    tmp = J * (S[i + 1][j] + S[i - 1][j] + S[i][j + 1] + S[i][j - 1])
    tmp2 = D * (np.cross(S[i + 1][j], x) + np.cross(S[i][j + 1], y) - np.cross(S[i - 1][j], x) - np.cross(S[i][j - 1], y))
    tmp3 = 2 * z * K * np.dot(z, S[i][j]).item()
    res = - tmp + tmp2 - tmp3
    return res

#Calculate energy
def E():
    res = 0
    for i in range(1, SX + 1):
        for j in range(1, SY + 1):
            tmp = J * np.dot((S[i + 1][j] + S[i - 1][j] + S[i][j + 1] + S[i][j - 1]), S[i][j])
            tmp2 = D * (+ np.dot(np.cross(S[i + 1][j], S[i][j]), x)
                        + np.dot(np.cross(S[i][j + 1], S[i][j]), y)
                        - np.dot(np.cross(S[i - 1][j], S[i][j]), x)
                        - np.dot(np.cross(S[i][j - 1], S[i][j]), y))
            tmp3 = K * (np.dot(z, S[i][j]) ** 2)
            res = res - tmp / 2 - tmp2 / 2 - tmp3
    return res

#Calculate energy
def E2():
    res = 0
    for i in range(1, SX + 1):
        for j in range(1, SY + 1):
            res += np.dot(S[i][j], grad(i,j))
    return res * 0.5

#Normalise each vector
def normalize():
    for i in range(1, SX + 1):
        for j in range(1, SY + 1):
            norm = S[i][j][0] * S[i][j][0] + S[i][j][1] * S[i][j][1] + S[i][j][2] * S[i][j][2]
            norm = np.sqrt(norm)
            S[i][j][0] = S[i][j][0] / norm
            S[i][j][1] = S[i][j][1] / norm
            S[i][j][2] = S[i][j][2] / norm

normalize()

prev_grad =  np.random.randn(SX + 2, SY + 2, 3)
cur_grad = np.random.randn(SX + 2, SY + 2, 3)
for i in range(1, SX + 1):
    for j in range(1, SY + 1):
        prev_grad[i][j] = grad(i, j)

for k in range(0, 10000):
    newS = np.zeros_like(S)
    maxNorm = 0
    sum_of_prev_grad = 0
    sum_of_grad  =  0
    for i in range(1, SX + 1):
        for j in range(1, SY + 1):
            cur_grad[i][j] = grad(i,  j)

    for i in range(1, SX + 1):
        for j in range(1, SY + 1):
            sum_of_prev_grad = sum_of_prev_grad + np.dot(prev_grad[i][j], prev_grad[i][j])

    for i in range(1, SX + 1):
        for j in range(1, SY + 1):
            sum_of_grad = sum_of_grad + np.dot(cur_grad[i][j], cur_grad[i][j])

    alpha = sum_of_grad / sum_of_prev_grad

    for i in range(1, SX + 1):
        for j in range(1, SY + 1):
            #Calculte gradient
            g = grad(i,  j)

            #Calculate projection ofgradientto sphere
            projGradOnS = np.dot(S[i][j], g)
            g = g - projGradOnS * S[i][j]
            maxNorm = np.maximum(maxNorm, np.linalg.norm(g))
            prev_grad[i][j] =grad(i, j)
            #Make step in gradient direction
            newS[i][j] = S[i][j] - alpha * grad(i, j)

    S = newS
    normalize()

print(E())

 \end{python}
\subsection{Метод 3}
 Приведу реализацию метода сопряженных градиентов Флетчера - Ривза
 \begin{python}
import numpy as np

# matrix_size = 16


SX = 4
SY = 4
x = np.array([1.0, 0.0, 0.0])
y = np.array([0.0, 1.0, 0.0])
z = np.array([0.0, 0.0, 1.0])
S = np.random.randn(SX + 2, SY + 2, 3)
for i in range(0, SX + 2):
    S[i][0] = np.array([0, 0, 0])
    S[i][SY + 1] = np.array([0, 0, 0])

for i in range(0, SY + 2):
    S[0][i] = np.array([0, 0, 0])
    S[SX + 1][i] = np.array([0, 0, 0])

K = 0.4
D = 0.35
J = 1
step = 0.1

# print(S.shape)
# np.roll
# np.cross
# alpha
# steps = 1000
def normalPrintS():
    for i in range(0, SX + 2):
        print()
        for j in range(0, SY + 2):
            print(S[i][j], end=" ")
    print()


def grad(i, j):
    tmp = J * (S[i + 1][j] + S[i - 1][j] + S[i][j + 1] + S[i][j - 1])
    tmp2 = D * (np.cross(S[i + 1][j], x) + np.cross(S[i][j + 1], y) - np.cross(S[i - 1][j], x) - np.cross(S[i][j - 1], y))
    tmp3 = 2 * z * K * np.dot(z, S[i][j]).item()
    res = - tmp + tmp2 - tmp3
    return res

def gradX(X, i, j):
    tmp = J * (X[i + 1][j] + X[i - 1][j] + X[i][j + 1] + X[i][j - 1])
    tmp2 = D * (np.cross(X[i + 1][j], x) + np.cross(X[i][j + 1], y) - np.cross(X[i - 1][j], x) - np.cross(X[i][j - 1], y))
    tmp3 = 2 * z * K * np.dot(z, X[i][j]).item()
    res = - tmp + tmp2 - tmp3
    return res



def E():
    res = 0
    for i in range(1, SX + 1):
        for j in range(1, SY + 1):
            tmp = J * np.dot((S[i + 1][j] + S[i - 1][j] + S[i][j + 1] + S[i][j - 1]), S[i][j])
            tmp2 = D * (+ np.dot(np.cross(S[i + 1][j], S[i][j]), x)
                        + np.dot(np.cross(S[i][j + 1], S[i][j]), y)
                        - np.dot(np.cross(S[i - 1][j], S[i][j]), x)
                        - np.dot(np.cross(S[i][j - 1], S[i][j]), y))
            tmp3 = K * (np.dot(z, S[i][j]) ** 2)
            res = res - tmp / 2 - tmp2 / 2 - tmp3
    return res

def E2():
    res = 0
    for i in range(1, SX + 1):
        for j in range(1, SY + 1):
            res += np.dot(S[i][j], grad(i,j))
    return res * 0.5

def normalize():
    for i in range(1, SX + 1):
        for j in range(1, SY + 1):
            norm = S[i][j][0] * S[i][j][0] + S[i][j][1] * S[i][j][1] + S[i][j][2] * S[i][j][2]
            norm = np.sqrt(norm)
            S[i][j][0] = S[i][j][0] / norm
            S[i][j][1] = S[i][j][1] / norm
            S[i][j][2] = S[i][j][2] / norm

normalize()

prev_grad =  np.random.randn(SX + 2, SY + 2, 3)
cur_grad = np.random.randn(SX + 2, SY + 2, 3)
for i in range(1, SX + 1):
    for j in range(1, SY + 1):
        prev_grad[i][j] = grad(i, j)
omega = 1
k = 0
SinTheirCode = np.random.randn(SX + 2, SY + 2, 3)
for i in range(1, SX + 1):
    for j in range(1, SY + 1):
        SinTheirCode[i][j] = grad(i, j)


while(omega > 0.001):
    ch = 0
    zn = 0
    for i in range(1, SX + 1):
        for j in range(1, SY + 1):
            ch = ch + np.dot(grad(i,j), gradX(SinTheirCode, i, j))
            zn = zn + np.dot(gradX(SinTheirCode, i, j), SinTheirCode[i][j])

    beta = ch / zn
    for i in range(1, SX + 1):
        for j in range(1, SY + 1):
            SinTheirCode[i][j] = grad(i, j) + beta * SinTheirCode[i][j]
    ch = 0
    zn = 0
    for i in range(1, SX + 1):
        for j in range(1, SY + 1):
            ch = ch + np.dot(grad(i,j), SinTheirCode[i][j])
            zn = zn + np.dot(gradX(SinTheirCode, i, j), SinTheirCode[i][j])

    alpha  = - ch/zn

    maxNorm =  0
    for i in range(1, SX + 1):
        for j in range(1, SY + 1):
            ##
            g = grad(i, j)
            projGradOnS = np.dot(S[i][j], g)
            g = g - projGradOnS * S[i][j]
            maxNorm = np.maximum(maxNorm, np.linalg.norm(g))
            ##
            S[i][j] = S[i][j] - alpha * SinTheirCode[i][j]
    normalize()
print(E())

 \end{python}

\subsection{Метод 4}

 \begin{python}

SX = 5
SY = 5
x = np.array([1.0, 0.0, 0.0])
y = np.array([0.0, 1.0, 0.0])
z = np.array([0.0, 0.0, 1.0])
S = np.random.randn(SX + 2, SY + 2, 3)
for i in range(0, SX + 2):
    S[i][0] = np.array([0, 0, 0])
    S[i][SY + 1] = np.array([0, 0, 0])

for i in range(0, SY + 2):
    S[0][i] = np.array([0, 0, 0])
    S[SX + 1][i] = np.array([0, 0, 0])

K = 0.4
D = 0.35
J = 1
step = 0.1

# print(S.shape)
# np.roll
# np.cross
# alpha
# steps = 1000
def normalPrintS():
    for i in range(0, SX + 2):
        print()
        for j in range(0, SY + 2):
            print(S[i][j], end=" ")
    print()


def grad(i, j):
    tmp = J * (S[i + 1][j] + S[i - 1][j] + S[i][j + 1] + S[i][j - 1])
    tmp2 = D * (np.cross(S[i + 1][j], x) + np.cross(S[i][j + 1], y) - np.cross(S[i - 1][j], x) - np.cross(S[i][j - 1], y))
    tmp3 = 2 * z * K * np.dot(z, S[i][j]).item()
    res = - tmp + tmp2 - tmp3
    return res


def E():
    res = 0
    for i in range(1, SX + 1):
        for j in range(1, SY + 1):
            tmp = J * np.dot((S[i + 1][j] + S[i - 1][j] + S[i][j + 1] + S[i][j - 1]), S[i][j])
            tmp2 = D * (+ np.dot(np.cross(S[i + 1][j], S[i][j]), x)
                        + np.dot(np.cross(S[i][j + 1], S[i][j]), y)
                        - np.dot(np.cross(S[i - 1][j], S[i][j]), x)
                        - np.dot(np.cross(S[i][j - 1], S[i][j]), y))
            tmp3 = K * (np.dot(z, S[i][j]) ** 2)
            res = res - tmp / 2 - tmp2 / 2 - tmp3
    return res

def E2():
    res = 0
    for i in range(1, SX + 1):
        for j in range(1, SY + 1):
            res += np.dot(S[i][j], grad(i,j))
    return res * 0.5

def normalize():
    for i in range(1, SX + 1):
        for j in range(1, SY + 1):
            norm = S[i][j][0] * S[i][j][0] + S[i][j][1] * S[i][j][1] + S[i][j][2] * S[i][j][2]
            norm = np.sqrt(norm)
            S[i][j][0] = S[i][j][0] / norm
            S[i][j][1] = S[i][j][1] / norm
            S[i][j][2] = S[i][j][2] / norm

normalize()

prev_grad =  np.random.randn(SX + 2, SY + 2, 3)
cur_grad = np.random.randn(SX + 2, SY + 2, 3)
for i in range(1, SX + 1):
    for j in range(1, SY + 1):
        prev_grad[i][j] = grad(i, j)
omega = 1
k = 0
while(omega > 0.001):
    newS = np.zeros_like(S)
    maxNorm = 0
    sum_of_prev_grad = 0
    sum_of_grad  =  0
    for i in range(1, SX + 1):
        for j in range(1, SY + 1):
            cur_grad[i][j] = grad(i,  j)

    for i in range(1, SX + 1):
        for j in range(1, SY + 1):
            sum_of_prev_grad = sum_of_prev_grad + np.dot(prev_grad[i][j], prev_grad[i][j])

    for i in range(1, SX + 1):
        for j in range(1, SY + 1):
            sum_of_grad = sum_of_grad + np.dot(cur_grad[i][j], cur_grad[i][j])

    omega = sum_of_grad / sum_of_prev_grad

    for i in range(1, SX + 1):
        for j in range(1, SY + 1):
            g = grad(i,  j)
            projGradOnS = np.dot(S[i][j], g)
            g = g - projGradOnS * S[i][j]
            maxNorm = np.maximum(maxNorm, np.linalg.norm(g))
            if(k == 0):
                prev_grad[i][j] = grad(i, j)
                newS[i][j] = S[i][j] - step * (grad(i, j))
            else:
                newS[i][j] = S[i][j] - step * (grad(i, j)) + ( step * omega *  prev_grad[i][j] )
                prev_grad[i][j] = grad(i, j) + prev_grad[i][j] * omega
    S = newS
    normalize()
    print(E(),maxNorm)
    k = k + 1
print(E())

 \end{python}

\newpage
\section{Глава 3\\
Сравнение быстродействия методов}
 Сравним быстродействие данных методов:\\
 \begin{tikzpicture}
\begin{axis}[
    title={Метод 1},
    xlabel={N},
   ylabel={time, c},
    xmin=3, xmax=10,
    ymin=0, ymax=120,
    xtick={4,5,6,7,8},
    ytick={0,20,40,60,80,100,120},
    legend pos=north west,
    ymajorgrids=true,
    grid style=dashed,
]

\addplot[
    color=blue,
    mark=square,
    ]
    coordinates {
    (0,23.1)(10,27.5)(20,32)(30,37.8)(40,44.6)(60,61.8)(80,83.8)(100,114)
    };
    \legend{Время работы для разных $n$}

\end{axis}
\end{tikzpicture}\\
 \begin{tikzpicture}
\begin{axis}[
    title={Метод 2},
    xlabel={N},
    ylabel={time, c},
    xmin=3, xmax=10,
    ymin=0, ymax=120,
    xtick={4,5,6,7,8},
    ytick={0,20,40,60,80,100,120},
    legend pos=north west,
    ymajorgrids=true,
    grid style=dashed,
]

\addplot[
    color=blue,
    mark=square,
    ]
    coordinates {
    (0,23.1)(10,27.5)(20,32)(30,37.8)(40,44.6)(60,61.8)(80,83.8)(100,114)
    };
    \legend{Время работы для разных $n$}

\end{axis}
\end{tikzpicture}\\

 \begin{tikzpicture}
 \begin{axis}[
    title={Метод 3},
    xlabel={N},
    ylabel={time, c},
    xmin=3, xmax=10,
    ymin=0, ymax=120,
    xtick={4,5,6,7,8},
    ytick={0,20,40,60,80,100,120},
    legend pos=north west,
    ymajorgrids=true,
    grid style=dashed,
]

\addplot[
    color=blue,
    mark=square,
    ]
    coordinates {
    (0,23.1)(10,27.5)(20,32)(30,37.8)(40,44.6)(60,61.8)(80,83.8)(100,114)
    };
    \legend{Время работы для разных $n$}

\end{axis}
\end{tikzpicture}\\

 \begin{tikzpicture}
\begin{axis}[
    title={Метод 4},
    xlabel={N},
    ylabel={time, c},
    xmin=3, xmax=10,
    ymin=0, ymax=120,
    xtick={4,5,6,7,8},
    ytick={0,20,40,60,80,100,120},
    legend pos=north west,
    ymajorgrids=true,
    grid style=dashed,
]

\addplot[
    color=blue,
    mark=square,
    ]
    coordinates {
    (0,23.1)(10,27.5)(20,32)(30,37.8)(40,44.6)(60,61.8)(80,83.8)(100,114)
    };
    \legend{Время работы для разных $n$ }

\end{axis}
\end{tikzpicture}\\

 \begin{tikzpicture}
\begin{axis}[
    title={Метод 5},
    xlabel={N},
    ylabel={time, c},
    xmin=3, xmax=10,
    ymin=0, ymax=120,
    xtick={4,5,6,7,8},
    ytick={0,20,40,60,80},
    legend pos=north west,
    ymajorgrids=true,
    grid style=dashed,
]

\addplot[
    color=blue,
    mark=square,
    ]
    coordinates {
    (4,23.1)(5,27.5)(6,32)(7,37.8)(8,44.6)
    };
    \legend{CuSO$_4\cdot$5H$_2$O}
\end{axis}
\end{tikzpicture}\\
Исходя из показанных сверху граифков, можно заключить что самым оптимальным методом является метод      , а остальные не такими оптимальными.
Однако, расход памяти  разных алгоритмах тоже разнится.
 \newpage
    \section{Заключение}
    В данной работе мною была достигнута цель исследования, а именно выяснение наиболее оптимального метода решения этой задачи, приведен  код решения данных задач, проведен их анализ по времени работы.
    Как можно заметить, все перечисленные мною методы решают поставленную задачу, с той или иной скоростью. Соответственное, при решении данной задачи можно пользоваться любым из них.
    В дальнейшем, можно постараться оптимизировать текущие агоритмы  путем выбора более подходящих $\alpha$, или использования координально других алгоритмов , например стохастических, что , возможно, даст прибавку в скорости, или попытаться каким то образом оптимизировать по памяти,снизив ее потребление.
\newpage
\section{Литература}
1)Fast and Robust Algorithm for the Energy Minimization of Spin Systems Applied in
an Analysis of High Temperature Spin Configurations in Terms of Skyrmion Density, Aleksei V. Ivanov,1, 2, ∗ Valery M. Uzdin,2, 3 and Hannes J´onsson\\
2)М.Э.АББАСОВ
МЕТОДЫ ОПТИМИЗАЦИИ\\
3)https://numpy.org/doc/1.18/user/basics.types.html\\
4)https://docs.python.org/3/download.html\\
5)http://www.machinelearning.ru/wiki/index.php?title=Метод\_сопряжённых\_градиентов
\end{document}
